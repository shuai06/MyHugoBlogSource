---
title: ä¸¢å¼ƒæ³•(dropout)
comments: true
mathjax: true
date: 2022-09-13 10:28:55
tags: ["ä¸¢å¼ƒæ³•", "dropout"]
categories:
- AI
cover:
top_img:
description:
---
<script type="text/javascript" src="/js/src/bai.js"></script>


>é™¤äº†å‰ä¸€èŠ‚ä»‹ç»çš„æƒé‡è¡°å‡ä»¥å¤–ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹å¸¸å¸¸ä½¿ç”¨ä¸¢å¼ƒæ³•ï¼ˆdropoutï¼‰æ¥åº”å¯¹è¿‡æ‹Ÿåˆé—®é¢˜

æ ‡å‡†å¤„ç†æµç¨‹ï¼š
- $â„=Ïƒ(ğ‘Š1ğ‘¥+ğ‘1)$  ç¬¬ä¸€ä¸ªéšè—å±‚
- $â„â€²=ğ‘‘ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡(â„)$  è¿›è¡Œéšæœºçš„åˆ é™¤
- $ğ‘œ=ğ‘Š2â„â€²+ğ‘2$  ç¬¬äºŒå±‚è¾“å‡º
- $ğ‘¦=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘œ)$  softmaxå±‚è¾“å‡º


- åœ¨**æµ‹è¯•æ¨¡å‹**æ—¶ï¼Œæˆ‘ä»¬ä¸ºäº†æ‹¿åˆ°æ›´åŠ ç¡®å®šæ€§çš„ç»“æœï¼Œä¸€èˆ¬**ä¸ä½¿ç”¨ä¸¢å¼ƒæ³•**ã€‚




## å®ç°
```python
import torch
from torch import nn
from d2l import d2ltorch as d2l


# å®šä¹‰ä¸¢å¼ƒå‡½æ•°
# å°†ä»¥dropoutçš„æ¦‚ç‡ä¸¢å¼ƒXä¸­çš„å…ƒç´ 
def dropout_layer(X,dropout):
    assert 0 <= dropout <=1 # æ¦‚ç‡è‚¯å®šæ˜¯åœ¨0-1ä¹‹é—´
    if dropout ==1:
        return torch.zeros_like(X)
    # è¿™ç§æƒ…å†µä¸‹æŠŠå…¨éƒ¨å…ƒç´ éƒ½ä¸¢å¼ƒ
    if dropout ==0:
        return X
    # maskçŸ©é˜µä¸º0-1ä¹‹é—´çš„å‡åŒ€éšæœºåˆ†å¸ƒï¼Œå¤§äºdropoutä¸º1å°äºä¸º0
    mask = (torch.randn(X.shape)>dropout).float()
    # ä¼˜å…ˆä½¿ç”¨ä¹˜æ³•è€Œä¸æ˜¯X[mask]=0ï¼Œä¹˜æ³•é€Ÿåº¦è¿œå¤§äºé€‰å–
    return mask*X/(1.0-dropout)

# æµ‹è¯•ä¸€ä¸‹
# torch.tensor.uniformä»å‡åŒ€åˆ†å¸ƒä¸­æŠ½æ ·æ•°å€¼è¿›è¡Œå¡«å……  
# torch.zeros(3,3).uniform_(0,1)

X= torch.arange(16, dtype = torch.float32).reshape((2, 8))
# print(X)
# print(dropout_layer(X, 0.))
print(dropout_layer(X, 0.5))
# print(dropout_layer(X, 1.))


# å®šä¹‰å…·æœ‰ä¸¤ä¸ªéšè—å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºï¼Œæ¯ä¸ªéšè—å±‚åŒ…å«256ä¸ªå•å…ƒ
num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 128
dropout1, dropout2 = 0.2, 0.5
class Net(nn.Module): # (object)å†™æ³•æ˜¯ç»§æ‰¿ï¼Œè‡ªå·±å›é¡¾ä¸€ä¸‹è€ç”·å­©çš„è¯¾ç¨‹
    def __init__(self,num_inputs,num_outputs,num_hiddens1,num_hiddens2,is_training = True):
        # ä½¿ç”¨superæ–¹æ³•å¯ä»¥é‡æ–°è°ƒç”¨çˆ¶ç±»ä¸­çš„å‡½æ•°
        # python3 ç›´æ¥å†™æˆ ï¼š super().__init__()
        # python2 å¿…é¡»å†™æˆ ï¼šsuper(æœ¬ç±»å,self).__init__()
        super(Net,self).__init__()
        self.num_inputs = num_inputs
        self.training = is_training
        # éšè—å±‚çš„è®¾ç½®ï¼Œéƒ½æ˜¯çº¿æ€§å±‚ï¼Œæ³¨æ„åšå¥½å±‚æ•°çš„å‰åè¿æ¥
        self.lin1 = nn.Linear(num_inputs,num_hiddens1)
        self.lin2 = nn.Linear(num_hiddens1,num_hiddens2)
        self.lin3 = nn.Linear(num_hiddens2,num_outputs)
        self.relu = nn.ReLU()
    
    def forward(self,X):
        H1 = self.relu(self.lin1(X.reshape(-1,self.num_inputs))) # ç¬¬ä¸€éšè—å±‚
        # åªæœ‰åœ¨è®­ç»ƒæ¨¡å‹æ—¶æ‰ä½¿â½¤dropout
        if self.training == True:
            # åœ¨ç¬¬â¼€ä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ â¼€ä¸ªdropoutå±‚
            H1 = dropout_layer(H1, dropout1) 
        H2 = self.relu(self.lin2(H1)) # ç¬¬äºŒéšè—å±‚
        if self.training == True:
            # åœ¨ç¬¬â¼†ä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ â¼€ä¸ªdropoutå±‚
            H2 = dropout_layer(H2, dropout2)
        out = self.lin3(H2) # è¾“å‡ºå±‚ä¸éœ€è¦æ¿€æ´»å‡½æ•°
        return out
net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)


## è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹
num_epochs, lr, batch_size = 5, 100.0, 256
loss = torch.nn.CrossEntropyLoss()
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)




```


## ç®€å•å®ç°
åœ¨PyTorchä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨å…¨è¿æ¥å±‚åæ·»åŠ Dropoutå±‚å¹¶æŒ‡å®šä¸¢å¼ƒæ¦‚ç‡ã€‚
åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼ŒDropoutå±‚å°†ä»¥æŒ‡å®šçš„ä¸¢å¼ƒæ¦‚ç‡éšæœºä¸¢å¼ƒä¸Šä¸€å±‚çš„è¾“å‡ºå…ƒç´ ï¼›åœ¨æµ‹è¯•æ¨¡å‹æ—¶ï¼ˆå³model.eval()åï¼‰ï¼ŒDropoutå±‚å¹¶ä¸å‘æŒ¥ä½œç”¨ã€‚
```python
net = nn.Sequential(nn.Flatten(),
    nn.Linear(784, 256),
    nn.ReLU(),
    # åœ¨ç¬¬â¼€ä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ â¼€ä¸ªdropoutå±‚
    nn.Dropout(dropout1),
    nn.Linear(256, 128),
    nn.ReLU(),
    # åœ¨ç¬¬â¼†ä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ â¼€ä¸ªdropoutå±‚
    nn.Dropout(dropout2),
    nn.Linear(128, 10))
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)
net.apply(init_weights)


optimizer = torch.optim.SGD(net.parameters(), lr=0.5)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)

```

- æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ä¸¢å¼ƒæ³•åº”å¯¹è¿‡æ‹Ÿåˆã€‚
- ä¸¢å¼ƒæ³•åªåœ¨è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨ã€‚